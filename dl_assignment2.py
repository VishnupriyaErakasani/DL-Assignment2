# -*- coding: utf-8 -*-
"""DL  ASSIGNMENT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11HI9ogGd1A0tyy-f8gYM5iJbFP-SKJ0B
"""

# @title Question1
import tarfile
import os

# Path to the downloaded tar file
tar_path = "/content/dakshina_dataset_v1.0.tar"

# Extract it to a folder
extracted_path = "/content/dakshina_dataset_v1.0"

# Only extract if not already done
if not os.path.exists(extracted_path):
    with tarfile.open(tar_path) as tar:
        tar.extractall(path="/content/")

# Now access the lexicon files properly
data_dir = os.path.join(extracted_path, "hi")

train_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv")
val_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv")
test_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv")

# Function to load the data
def load_data(path):
    import pandas as pd
    data = pd.read_csv(path, sep="\t", header=None)
    data.columns = ['hi', 'latin', 'count']
    return data

# Now load the data
train_data = load_data(train_path)
val_data = load_data(val_path)
test_data = load_data(test_path)

import os
import pandas as pd

# Change this path as needed
data_dir = "/content/dakshina_dataset_v1.0.tar"

# Training, validation and test files
train_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv")
val_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv")
test_path = os.path.join(data_dir, "/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv")

# Load data
def load_data(path):
    data = pd.read_csv(path, sep="\t", header=None)
    data.columns = ['hi', 'latin', 'count']  # Match the number of columns
    return data[['hi', 'latin']]  # Use only the two needed

train_data = load_data(train_path)
val_data = load_data(val_path)
test_data = load_data(test_path)

def clean_data(data, source_col='latin', target_col='hi'):
    # Drop rows with missing source or target
    data = data.dropna(subset=[source_col, target_col])
    # Ensure all values are strings
    data[source_col] = data[source_col].astype(str)
    data[target_col] = data[target_col].astype(str)
    return data

train_data = clean_data(train_data)
val_data = clean_data(val_data)
test_data = clean_data(test_data)

def create_vocab(data, source_col, target_col):
    source_vocab = set()
    target_vocab = set()

    for _, row in data.iterrows():
        source_vocab.update(list(row[source_col]))
        target_vocab.update(list(row[target_col]))

    source_vocab = sorted(list(source_vocab))
    target_vocab = sorted(list(target_vocab))

    source_token_index = {char: i+1 for i, char in enumerate(source_vocab)}  # 0 = padding
    target_token_index = {char: i+1 for i, char in enumerate(target_vocab)}

    reverse_target_token_index = {i: char for char, i in target_token_index.items()}

    return source_token_index, target_token_index, reverse_target_token_index

import string
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

def create_vocab(data, source_col, target_col):
    source_vocab = set()
    target_vocab = set()

    for _, row in data.iterrows():
        source_vocab.update(list(row[source_col]))
        target_vocab.update(list(row[target_col]))

    source_vocab = sorted(list(source_vocab))
    target_vocab = sorted(list(target_vocab))

    source_token_index = {char: i+1 for i, char in enumerate(source_vocab)}  # reserve 0 for padding
    target_token_index = {char: i+1 for i, char in enumerate(target_vocab)}

    reverse_target_token_index = {i: char for char, i in target_token_index.items()}

    return source_token_index, target_token_index, reverse_target_token_index

source_token_index, target_token_index, reverse_target_token_index = create_vocab(train_data, 'latin', 'hi')

num_encoder_tokens = len(source_token_index) + 1
num_decoder_tokens = len(target_token_index) + 1

max_encoder_seq_length = max([len(txt) for txt in train_data['latin']])
max_decoder_seq_length = max([len(txt) for txt in train_data['hi']]) + 2  # +2 for start/end tokens

def create_vocab(data, source_col, target_col):
    source_vocab = set()
    target_vocab = set()

    for _, row in data.iterrows():
        source_vocab.update(list(row[source_col]))
        target_vocab.update(list(row[target_col]))

    # Add start and end tokens for decoder vocab
    target_vocab.update(['\t', '\n'])

    source_vocab = sorted(list(source_vocab))
    target_vocab = sorted(list(target_vocab))

    # Assign index starting from 0 for padding
    source_token_index = {char: i for i, char in enumerate(source_vocab)}  # Padding at index 0
    target_token_index = {char: i for i, char in enumerate(target_vocab)}   # Padding at index 0

    reverse_target_token_index = {i: char for char, i in target_token_index.items()}

    return source_token_index, target_token_index, reverse_target_token_index

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

def vectorize(data, source_token_index, target_token_index, max_encoder_len, max_decoder_len):
    encoder_input_data = []
    decoder_input_data = []
    decoder_target_data = []

    for _, row in data.iterrows():
        input_text = row['latin']
        target_text = row['hi']

        # Add start and end tokens
        target_text = '\t' + target_text + '\n'

        # Encoder sequence
        encoder_seq = [source_token_index[char] for char in input_text]

        # Decoder input and output
        decoder_seq = [target_token_index[char] for char in target_text]

        decoder_input_seq = decoder_seq[:-1]
        decoder_target_seq = decoder_seq[1:]

        encoder_input_data.append(encoder_seq)
        decoder_input_data.append(decoder_input_seq)
        decoder_target_data.append(decoder_target_seq)

    # Padding the sequences to a consistent length
    encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_encoder_len, padding='post')
    decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_decoder_len, padding='post')
    decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_decoder_len, padding='post')

    # One-hot encode the target data
    num_decoder_tokens = len(target_token_index)
    decoder_target_data = to_categorical(decoder_target_data, num_classes=num_decoder_tokens)

    return encoder_input_data, decoder_input_data, decoder_target_data

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, GRU, SimpleRNN, Dense

def build_model(cell_type='LSTM', embedding_dim=64, hidden_dim=128):
    # Inputs
    encoder_inputs = Input(shape=(None,))
    decoder_inputs = Input(shape=(None,))

    # Embeddings
    encoder_emb = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim)(encoder_inputs)
    decoder_emb = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim)(decoder_inputs)

    RNN_Cell = {'RNN': SimpleRNN, 'LSTM': LSTM, 'GRU': GRU}[cell_type]

    # Encoder
    if cell_type == 'LSTM':
        encoder_outputs, state_h, state_c = RNN_Cell(hidden_dim, return_state=True)(encoder_emb)
        encoder_states = [state_h, state_c]
    else:
        encoder_outputs, state_h = RNN_Cell(hidden_dim, return_state=True)(encoder_emb)
        encoder_states = [state_h]

    # Decoder
    if cell_type == 'LSTM':
        decoder_rnn = RNN_Cell(hidden_dim, return_sequences=True, return_state=True)
        decoder_outputs, _, _ = decoder_rnn(decoder_emb, initial_state=encoder_states)
    else:
        decoder_rnn = RNN_Cell(hidden_dim, return_sequences=True, return_state=True)
        decoder_outputs, _ = decoder_rnn(decoder_emb, initial_state=encoder_states)

    decoder_dense = Dense(num_decoder_tokens, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model


# Step 1: Vectorize the data again (correcting any tokenization issues)
encoder_input_data, decoder_input_data, decoder_target_data = vectorize(
    train_data, source_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length
)

# Ensure num_decoder_tokens is correct based on the actual target_token_index size
num_decoder_tokens = len(target_token_index)

# Step 2: Build the model
model = build_model(cell_type='LSTM', embedding_dim=64, hidden_dim=128)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 3: Fit the model
history = model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=10,
    validation_split=0.1
)

# Optionally, plot training history
import matplotlib.pyplot as plt
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input

# Define encoder model for inference
encoder_inputs_inf = model.input[0]  # encoder input from the original model
encoder_emb_layer = model.layers[2]  # embedding layer
encoder_rnn = model.layers[4]        # LSTM layer

# Get encoder states
encoder_emb_inf = encoder_emb_layer(encoder_inputs_inf)
_, state_h_enc, state_c_enc = encoder_rnn(encoder_emb_inf)
encoder_states = [state_h_enc, state_c_enc]

encoder_model = Model(encoder_inputs_inf, encoder_states)

# Decoder setup
decoder_inputs_inf = model.input[1]  # decoder input from the original model
decoder_emb_layer = model.layers[3]
decoder_rnn = model.layers[5]
decoder_dense = model.layers[6]

# New inputs for hidden states at inference
decoder_state_input_h = Input(shape=(128,))
decoder_state_input_c = Input(shape=(128,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Get embeddings and run through RNN
decoder_emb_inf = decoder_emb_layer(decoder_inputs_inf)
decoder_outputs_inf, state_h_inf, state_c_inf = decoder_rnn(
    decoder_emb_inf, initial_state=decoder_states_inputs
)
decoder_states_inf = [state_h_inf, state_c_inf]

# Final output layer
decoder_outputs_inf = decoder_dense(decoder_outputs_inf)

# Build decoder model
decoder_model = Model(
    [decoder_inputs_inf] + decoder_states_inputs,
    [decoder_outputs_inf] + decoder_states_inf
)

# Example input text
example_input = "namaste"

# Function to encode the Latin input into a padded sequence
def encode_input(text):
    seq = [source_token_index.get(char, 0) for char in text]  # default to 0 if char not found
    return pad_sequences([seq], maxlen=max_encoder_seq_length, padding='post')

# Function to decode the sequence using inference
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)

    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_token_index['\t']  # Start token

    decoded_sentence = ''
    stop_condition = False
    while not stop_condition:
        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)

        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_token_index[sampled_token_index]
        decoded_sentence += sampled_char

        if (sampled_char == '\n' or len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = True

        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

    return decoded_sentence.strip()

# Encode and predict
input_seq = encode_input(example_input)
predicted_output = decode_sequence(input_seq)

# Show the result
print(f"Latin Input: {example_input}")
print(f"Predicted Devanagari Transliteration: {predicted_output}")

def predict_examples(examples):
    for input_text in examples:
        input_seq = [source_token_index[char] for char in input_text if char in source_token_index]
        input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')
        decoded = decode_sequence(input_seq)
        print(f"Latin: {input_text} → Devanagari: {decoded}")

sample_latin_words = [
    "namaste",
    "krishna",
    "bharat",
    "dilli",
    "shakti",
    "mantri",
    "prem",
    "ramayan",
    "yatra",
    "vishwa"
]

predict_examples(sample_latin_words)

def evaluate_model(test_data):
    correct = 0
    total = 0
    for _, row in test_data.iterrows():
        input_text = row['latin']
        true_text = row['hi']
        input_seq = encode_input(input_text)
        pred_text = decode_sequence(input_seq)

        if pred_text == true_text:
            correct += 1
        total += 1

    accuracy = correct / total
    print(f"Accuracy on test set: {accuracy * 100:.2f}%")
    return accuracy

evaluate_model(test_data.sample(100))



# @title Question2
import zipfile
import os

file_path = '/content/archive.zip'
extract_dir = '/content/extracted_data'  # Choose a directory to extract to

try:
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print(f"Successfully extracted '{file_path}' to '{extract_dir}'")

    # Now you can explore the extracted data
    for root, dirs, files in os.walk(extract_dir):
        print(f"Directory: {root}")
        for file in files:
            print(f"  File: {file}")

except FileNotFoundError:
    print(f"Error: File not found at '{file_path}'")
except zipfile.BadZipFile:
    print(f"Error: Could not open or read the zip file '{file_path}'. It might be corrupted.")
except Exception as e:
    print(f"An error occurred: {e}")

pip install transformers datasets

from datasets import load_dataset

def load_lyrics_from_text(file_paths):
    lines = []
    for file_path in file_paths:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:  # Ignore empty lines
                    lines.append(line)
    return {"text": lines}

# Replace with the actual path(s) to your lyric files
lyric_file_paths = ['/content/extracted_data/csv/JustinBieber.csv', '/content/extracted_data/json files/Lyrics_JustinBieber.json']
raw_dataset = load_dataset("text", data_files=lyric_file_paths)

print(raw_dataset)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add a padding token if it doesn't exist (important for batching)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

block_size = 128  # Adjust this based on your GPU memory and desired sequence length

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length', max_length=block_size)

tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, num_proc=4) # Adjust num_proc for parallel processing

# Remove the original text column as the model will use the tokenized input
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

from transformers import TrainingArguments

output_dir = "./gpt2-lyrics"  # Directory to save the fine-tuned model
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=3,      # Adjust the number of training epochs
    per_device_train_batch_size=4, # Adjust batch size based on your GPU memory
    save_steps=1000,
    save_total_limit=2,
    prediction_loss_only=True,
    learning_rate=5e-5,
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"], # Assuming your raw_dataset has a 'train' split
    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),
                               'attention_mask': torch.stack([f['attention_mask'] for f in data]),
                               'labels': torch.stack([f['input_ids'] for f in data])}, # Labels are the input_ids for LM
)

trainer.train()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset
import os

block_size = 128  # Adjust as needed
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length', max_length=block_size)

tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, num_proc=os.cpu_count()) # Use available CPUs
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["main"] if "main" in tokenized_dataset else tokenized_dataset["train"],
    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),
                               'attention_mask': torch.stack([f['attention_mask'] for f in data]),
                               'labels': torch.stack([f['input_ids'] for f in data])},
)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset
import os

# 1. Load Dataset (Ensure this is correctly loading your lyric data)
lyric_file_paths = ['/content/extracted_data/csv/JustinBieber.csv'] # Replace with your actual paths
raw_dataset = load_dataset("text", data_files=lyric_file_paths)
print(f"Raw dataset: {raw_dataset}")

# 2. Load Tokenizer and Model
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# 3. Preprocess Dataset
block_size = 128
def tokenize_function(examples):
    result = tokenizer(examples["text"], truncation=True, padding='max_length', max_length=block_size)
    return result

tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, num_proc=os.cpu_count())
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
print(f"Tokenized dataset: {tokenized_dataset}")

# Inspect a few examples in tokenized_dataset
print("Example from tokenized_dataset:")
print(tokenized_dataset["train"][0])
print(tokenized_dataset["train"][1])

# 4. Define Training Arguments
output_dir = "./gpt2-lyrics"
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=1, # Start with a small number for testing
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=1,
    prediction_loss_only=True,
    learning_rate=5e-5,
)

# 5. Create Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"], # Ensure "train" split exists
    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),
                               'attention_mask': torch.stack([f['attention_mask'] for f in data]),
                               'labels': torch.stack([f['input_ids'] for f in data])},
)

# 6. Train the Model
# trainer.train() # Keep this commented out for now

import torch
from transformers import Trainer

def data_collator(examples):
    input_ids = [example['input_ids'] for example in examples]
    attention_mask = [example['attention_mask'] for example in examples]
    labels = [example['input_ids'] for example in examples]  # For causal LM

    return {
        'input_ids': torch.tensor(input_ids),
        'attention_mask': torch.tensor(attention_mask),
        'labels': torch.tensor(labels)
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=data_collator, # Use the explicitly defined data_collator
)

# Now try training
trainer.train()

trainer.save_model("./fine-tuned-gpt2-lyrics")
tokenizer.save_pretrained("./fine-tuned-gpt2-lyrics")

print("Fine-tuning complete. Model saved to ./fine-tuned-gpt2-lyrics")

# 8. Generate Lyrics (Example Usage)
def generate_lyrics(prompt, max_length=200, num_return_sequences=3, temperature=0.8):
    finetuned_model = AutoModelForCausalLM.from_pretrained("./fine-tuned-gpt2-lyrics").to(model.device)
    finetuned_tokenizer = AutoTokenizer.from_pretrained("./fine-tuned-gpt2-lyrics")

    input_ids = finetuned_tokenizer.encode(prompt, return_tensors="pt").to(finetuned_model.device)

    output = finetuned_model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        temperature=temperature,
        pad_token_id=finetuned_tokenizer.eos_token_id,
        do_sample=True,
    )

    generated_lyrics = []
    for i in range(num_return_sequences):
        generated_text = finetuned_tokenizer.decode(output[i], skip_special_tokens=True)
        generated_lyrics.append(generated_text)
    return generated_lyrics

# Example usage
prompt = "The rain falls down on a lonely street,"
generated_lyrics = generate_lyrics(prompt)

print("\n--- Generated Lyrics ---")
for i, lyrics in enumerate(generated_lyrics):
    print(f"Generated Lyric {i+1}:\n{lyrics}\n---")